@article{doi:10.1177/14703572241247836,
author = {Enzo D’Armenio and Maria Giulia Dondero},
title = {Introduction: hyper-visuality: images in the era of social platforms, digital archives and computational economies},
journal = {Visual Communication},
volume = {23},
number = {3},
pages = {405–425},
year = {2024a},
doi = {10.1177/14703572241247836},
URL = {https://doi-org.crai.referencistas.com/10.1177/14703572241247836},
eprint = {https://doi-org.crai.referencistas.com/10.1177/14703572241247836},
abstract = {Images are today at the centre of multiple social and technological tensions as a consequence of the adoption of digital coding, of the massive diffusion of social networks and of the algorithmic processing to which they are subject, resulting in new opportunities for developing analytical inquiries and meaning-producing social actions. In this introduction, the authors intend to reconstruct the broad context that makes images one of the most important resources of the digital era and to focus on some of the research tracks that characterize it. In the first part, they begin by focusing on the relationship between images and the digital, which they retrace in accordance with the selection of three key moments: the transition from ontology to the epistemology of digital media; the opening, by social networks and portable devices, of a field for the computational study of contemporary cultures; and, finally, the analytical potential arising from the encounter between digital archives and computer algorithms. In the second part, they present the three axes around which this issue is structured: archives, identity and algorithms. They first of all discuss the concept of the archive, by presenting four different understandings it has come to bear in conjunction with digital encoding – the archive as heritage, resource, effect and as database. They go on to address the relation between images and identities, arguing that social platforms and visual apps are a new domain for identity experimentation and social aggregation. Finally, they discuss the issue of algorithms and more generally of the new computational economy that associates large amounts of data with their mobilization as operational images.}
}

@article{doi:10.1057/jit.2011.2,
author = {William H Dutton},
title = {The Politics of Next Generation Research Democratizing Research-Centred Computational Networks},
journal = {Journal of Information Technology},
volume = {26},
number = {2},
pages = {109–119},
year = {2011b},
doi = {10.1057/jit.2011.2},
URL = {https://doi-org.crai.referencistas.com/10.1057/jit.2011.2},
eprint = {https://doi-org.crai.referencistas.com/10.1057/jit.2011.2},
abstract = {Research on information technology has been focused primarily on the worlds of IT and management systems for business and government to the relative neglect of research on the digital and institutional infrastructures that underpin the research enterprise itself. When digital research is studied, the emphasis has been on the diffusion of technological innovations, rather than the social and political dynamics shaping the design and role of technologies in research. However, what researchers know, and with whom they collaborate, could be transformed through the strategic use of advances designed to support research, defined here as ‘research-centred computational networks’. This article presents a framework for conceptualizing the social and technological choices shaping the next generation of research in ways that could open – democratize – key aspects of the research process that move well beyond academic publication. The framework highlights the limited scope of innovation to date, and identifies a variety of factors that maintain and enhance institutional control over the research process, at the risk of losing the creative and productive bottom-up participation by networked researchers and citizen researchers among the public at large. Conceptualizing, prioritizing and advancing study of next generation research is one of the most significant but difficult challenges facing scholars of information technology.}
}

@article{doi:10.1068/p7275,
author = {Donald D Hoffman and Manish Singh},
title = {Computational Evolutionary Perception},
journal = {Perception},
volume = {41},
number = {9},
pages = {1073–1091},
year = {2012c},
doi = {10.1068/p7275},
note = {PMID:23409373},
URL = {https://doi-org.crai.referencistas.com/10.1068/p7275},
eprint = {https://doi-org.crai.referencistas.com/10.1068/p7275},
abstract = {Marr proposed that human vision constructs “a true description of what is there”. He argued that to understand human vision one must discover the features of the world it recovers and the constraints it uses in the process. Bayesian decision theory (BDT) is used in modern vision research as a probabilistic framework for understanding human vision along the lines laid out by Marr. Marr’s contribution to vision research is substantial and justly influential. We propose, however, that evolution by natural selection does not, in general, favor perceptions that are true descriptions of the objective world. Instead, research with evolutionary games shows that perceptual systems tuned solely to fitness routinely outcompete those tuned to truth. Fitness functions depend not just on the true state of the world, but also on the organism, its state, and the type of action. Thus, fitness and truth are distinct. Natural selection depends only on expected fitness. It shapes perceptual systems to guide fitter behavior, not to estimate truth. To study perception in an evolutionary context, we introduce the framework of Computational Evolutionary Perception (CEP). We show that CEP subsumes BDT, and reinterprets BDT as evaluating expected fitness rather than estimating truth.}
}

@article{doi:10.1177/1468087420931730,
author = {Avilash Jain and Anand Krishnasamy and Pradeep V},
title = {Computational optimization of reactivity controlled compression ignition combustion to achieve high efficiency and clean combustion},
journal = {International Journal of Engine Research},
volume = {22},
number = {7},
pages = {2213–2232},
year = {2021d},
doi = {10.1177/1468087420931730},
URL = {https://doi-org.crai.referencistas.com/10.1177/1468087420931730},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1468087420931730},
abstract = {One of the major limitations of reactivity controlled compression ignition is higher unburned hydrocarbon and carbon monoxide emissions and lower thermal efficiency at part load operating conditions. In the present study, a combined numerical approach using a commercial three-dimensional computational fluid dynamics code CONVERGE along with artificial neural network and genetic algorithm is presented to address the above limitation. A production light-duty diesel engine is modified to run in reactivity controlled compression ignition by replacing an existing mechanical fuel injection system with a flexible electronic port fuel injection and common rail direct injection systems. The injection schedules of port fuel injection and direct injection injectors are controlled using National Instruments port and direct injection driver modules. Upon validation of combustion and emission parameters, parametric investigations are carried out to establish the effects of direct-injected diesel fuel timing start of injection (SOI), premixed fuel ratio and intake charge temperature on the engine performance and emissions in reactivity controlled compression ignition. The results obtained show that the start of injection timing and intake charge temperature significantly influence combustion phasing, while the premixed fuel ratio controls mixture reactivity and combustion quality. By utilizing the data generated with the validated computational fluid dynamics models, the artificial neural network models are trained to predict the engine exhaust emissions and efficiency. The artificial neural network models for gross indicated efficiency and oxides of nitrogen (NOx) are then coupled with genetic algorithm to maximize gross indicated efficiency while keeping the NOx and soot emissions within Euro VI emission limits. By optimizing the start of injection timing, premixed fuel ratio and intake charge temperature simultaneously using the artificial neural network models coupled with genetic algorithm, 19% improvement in gross indicated efficiency, 60% and 64% reduction in hydrocarbon and carbon monoxide emissions, respectively, are obtained in reactivity controlled compression ignition compared to the baseline case.}
}

@article{doi:10.1243/095765005X31261,
author = {F Martelli and S Pazzi and V Michelassi},
title = {Automatic computational fluid dynamics-based procedure for the optimization of a centrifugal impeller},
journal = {Proceedings of the Institution of Mechanical Engineers, Part A: Journal of Power and Energy},
volume = {219},
number = {7},
pages = {549–557},
year = {2005e},
doi = {10.1243/095765005X31261},
URL = {https://doi-org.crai.referencistas.com/10.1243/095765005X31261},
eprint = {https://doi-org.crai.referencistas.com/10.1243/095765005X31261},
abstract = {Abstract A typical centrifugal impeller characterized by a low flow coefficient and cylindrical blades is redesigned by means of an intelligent automatic search program. The procedure consists of a feasible sequential quadratic programming algorithm (Fletcher, R. Practical Methods of optimization, 2000 (Wiley)) coupled to a lazy learning (LL) interpolator 1 to speed-up the process. The program is able to handle geometric constraints to reduce the computational effort devoted to the analysis of non-physical configurations. The objective function evaluator is an in-house developed structured computational fluid dynamics (CFD) code. The LL approx-imator is called each time the stored database can provide a sufficiently accurate performance estimate for a given geometry, thus reducing the effective CFD computations. The impeller is represented by 25 geometric parameters describing the vane in the meridional and s-0 planes, the blade thickness, and the leading edge shape. The optimization is carried out on the impeller design point maximizing the polytropic efficiency with nearly constant flow coefficient and polytropic head. The optimization is accomplished by maintaining unaltered those geometrical parameters which have to be kept fixed in order to make the impeller fit the original stage. The optimization, carried out on a cluster of 16 PCs, is self-learning and leads to a geometry presenting an increased design point efficiency. The program is completely general and can be applied to any component which can be described by a finite number of geometrical parameters and computed by any numerical instrument to provide performance indices. The work presented in this paper was done under the METHOD EC funded project for the implementation of new technologies for optimization of centrifugal compressors.}
}

@article{doi:10.1177/0954406217737104,
author = {Niloufar Motazedi and Matthew P Cartmell and Jem A Rongong},
title = {Extending the functionality of a symbolic computational dynamic solver by using a novel term-tracking method},
journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
volume = {232},
number = {19},
pages = {3439–3452},
year = {2018f},
doi = {10.1177/0954406217737104},
URL = {https://doi-org.crai.referencistas.com/10.1177/0954406217737104},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0954406217737104},
abstract = {Symbolic computational dynamic solvers are currently under development in order to provide new and powerful tools for modelling nonlinear dynamical systems. Such solvers consist of two parts; the core solver, which comprises an approximate analytical method based on perturbation, averaging, or harmonic balance, and a specialised term-tracker. A term-tracking approach has been introduced to provide a powerful new feature into computational approximate analytical solutions by highlighting the many mathematical connections that exist, but which are invariably lost through processing, between the physical model of the system, the solution procedure itself, and the final result which is usually expressed in equation form. This is achieved by a highly robust process of term-tracking, recording, and identification of all the symbolic mathematical information within the problem. In this paper, the novel source and evolution encoding method is introduced for the first time and an implementation in Mathematica is described through the development of a specialised algorithm.}
}

@article{doi:10.1177/0956797618823540,
author = {Conrad Perry and Marco Zorzi and Johannes C. Ziegler},
title = {Understanding Dyslexia Through Personalized Large-Scale Computational Models},
journal = {Psychological Science},
volume = {30},
number = {3},
pages = {386–395},
year = {2019g},
doi = {10.1177/0956797618823540},
note = {PMID:30730792},
URL = {https://doi-org.crai.referencistas.com/10.1177/0956797618823540},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0956797618823540},
abstract = {Learning to read is foundational for literacy development, yet many children in primary school fail to become efficient readers despite normal intelligence and schooling. This condition, referred to as developmental dyslexia, has been hypothesized to occur because of deficits in vision, attention, auditory and temporal processes, and phonology and language. Here, we used a developmentally plausible computational model of reading acquisition to investigate how the core deficits of dyslexia determined individual learning outcomes for 622 children (388 with dyslexia). We found that individual learning trajectories could be simulated on the basis of three component skills related to orthography, phonology, and vocabulary. In contrast, single-deficit models captured the means but not the distribution of reading scores, and a model with noise added to all representations could not even capture the means. These results show that heterogeneity and individual differences in dyslexia profiles can be simulated only with a personalized computational model that allows for multiple deficits.}
}

@article{doi:10.1177/0002716215569220,
author = {Dhavan V. Shah and Alex Hanna and Erik P. Bucy and Chris Wells and Vidal Quevedo},
title = {The Power of Television Images in a Social Media Age: Linking Biobehavioral and Computational Approaches via the Second Screen},
journal = {The ANNALS of the American Academy of Political and Social Science},
volume = {659},
number = {1},
pages = {225–245},
year = {2015h},
doi = {10.1177/0002716215569220},
URL = {https://doi-org.crai.referencistas.com/10.1177/0002716215569220},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0002716215569220},
abstract = {There is considerable controversy surrounding the study of presidential debates, particularly efforts to connect their content and impact. Research has long debated whether the citizenry reacts to what candidates say, how they say it, or simply how they appear. This study uses detailed coding of the first 2012 debate between Barack Obama and Mitt Romney to test the relative influence of the candidates’ verbal persuasiveness and nonverbal features on viewers’ “second screen” behavior—their use of computers, tablets, and mobile phones to enhance or extend the televised viewing experience. To examine these relationships, we merged two datasets: (1) a shot-by-shot content analysis coded for functional, tonal, and visual elements of both candidates’ communication behavior during the debate; and (2) corresponding real-time measures, synched and lagged, of the volume and sentiment of Twitter expression about Obama and Romney. We find the candidates’ facial expressions and physical gestures to be more consistent and robust predictors of the volume and valence of Twitter expression than candidates’ persuasive strategies, verbal utterances, and voice tone during the debate.}
}

@article{doi:10.1177/0960327108097689,
author = {LG Valerio},
title = {Tools for evidence-based toxicology: computational-based strategies as a viable modality for decision support in chemical safety evaluation and risk assessment},
journal = {Human & Experimental Toxicology},
volume = {27},
number = {10},
pages = {757–760},
year = {2008i},
doi = {10.1177/0960327108097689},
note = {PMID:19042961},
URL = {https://doi-org.crai.referencistas.com/10.1177/0960327108097689},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0960327108097689}
}

@article{doi:10.1068/p7327,
author = {William H Warren},
title = {Does This Computational Theory Solve the Right Problem? Marr, Gibson, and the Goal of Vision},
journal = {Perception},
volume = {41},
number = {9},
pages = {1053–1060},
year = {2012j},
doi = {10.1068/p7327},
note = {PMID:23409371},
URL = {https://doi-org.crai.referencistas.com/10.1068/p7327},
eprint = {https://doi-org.crai.referencistas.com/10.1068/p7327},
abstract = {David Marr’s book Vision attempted to formulate a thoroughgoing formal theory of perception. Marr borrowed much of the “computational” level from James Gibson: a proper understanding of the goal of vision, the natural constraints, and the available information are prerequisite to describing the processes and mechanisms by which the goal is achieved. Yet, as a research program leading to a computational model of human vision, Marr’s program did not succeed. This article asks why, using the perception of 3D shape as a morality tale. Marr presumed that the goal of vision is to recover a general-purpose Euclidean description of the world, which can be deployed for any task or action. On this formulation, vision is underdetermined by information, which in turn necessitates auxiliary assumptions to solve the problem. But Marr’s assumptions did not actually reflect natural constraints, and consequently the solutions were not robust. We now know that humans do not in fact recover Euclidean structure—rather, they reliably perceive qualitative shape (hills, dales, courses, ridges), which is specified by the second-order differential structure of images. By recasting the goals of vision in terms of our perceptual competencies, and doing the hard work of analyzing the information available under ecological constraints, we can reformulate the problem so that perception is determined by information and prior knowledge is unnecessary.}
}

