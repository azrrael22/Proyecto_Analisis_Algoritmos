@article{doi:10.1177/14680874221125538,
author = {Ghufran Alkhamis and Mickael Silva and Emre Cenker and Hong G Im},
title = {A computational assessment of flame speed correlation in an ultra-lean pre-chamber engine},
journal = {International Journal of Engine Research},
volume = {24},
number = {6},
pages = {2538–2551},
year = {2023a},
doi = {10.1177/14680874221125538},
URL = {https://doi-org.crai.referencistas.com/10.1177/14680874221125538},
eprint = {https://doi-org.crai.referencistas.com/10.1177/14680874221125538},
abstract = {Predictive modeling of pre-chamber combustion engines relies primarily on the correct description of laminar and turbulent flame speeds. For engineering applications, the correlations of the flame speeds with physical variables involve empirical constants that are valid for a limited range of operating conditions. The current work aims at assessing the significance of laminar flame speed prediction in the simulation of ultra-lean pre-chamber engine combustion operated with methane. Gülder’s empirical correlation for laminar flame speed was chosen as a reference and was further modified for equivalence ratio, pressure and temperature ranges beyond what it was originally derived for, in order to confirm the original hypothesis; the pressure and temperature dependence were adopted as a power-law correlation. Based on the computational results using the skeletal reaction mechanism, the correlation was modified better represent the flame speeds at ultra-lean engine conditions, using GRI 3.0 as a reference. The modified correlation for methane was implemented in CONVERGE, a three-dimensional computational fluid dynamics (CFD) solver, and the results were validated against the experimental data. In all cases, the original formulation of Peters’s turbulent flame speed correlation was used and was found to have insignificant effect on the conditions under study, confirming the importance of the accurate determination of the laminar flame speed that dominates over any turbulence corrections for high Karlovitz number effects. The flame topology was also investigated to provide insights into the observed pressure behavior among the tested cases. Finally, the relevant turbulent combustion regimes encountered in the pre-chamber combustion engine conditions were examined in the Borghi-Peters diagram, further confirming the findings of the study.}
}

@article{doi:10.1177/1469066719894969,
author = {Richard D Bowen and William HC Martin and Charles E Hudson and David J McAdoo},
title = {Experimental and computational evidence for C=O π-bonding in [CH2OH]+ and related oxonium ions},
journal = {European Journal of Mass Spectrometry},
volume = {26},
number = {3},
pages = {187–194},
year = {2020b},
doi = {10.1177/1469066719894969},
note = {PMID:31969004},
URL = {https://doi-org.crai.referencistas.com/10.1177/1469066719894969},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1469066719894969},
abstract = {The question of whether [CH2OH]+ should be described as the hydroxymethyl cation, +CH2OH, or protonated formaldehyde, CH2=OH+, is reconsidered in the light of experimental information and new computational evidence. Previous arguments that the charge distribution in [CH2OH]+ may be probed by considering the incremental stabilisation of [CH2OH]+ induced by homologation on carbon (to give [CH3CHOH]+) or oxygen (to produce [CH2OCH3]+) are critically examined. Cation stabilisation energies are shown to be better indicators of the nature of these oxonium ions. Further insight into the structure of larger CnH2n+1O+ oxonium ions is obtained by considering the site of protonation of enol ethers and related species. Computational information, including AIM (Atoms and Molecules) and NBA (Natural Bond Analysis) charges on the carbon and oxygen atoms in [CH2OH]+ and related species, is considered critically. Particular attention is focused on the calculated bond lengths and barriers to rotation about the C–O bond(s) in [CH2OH]+, [CH3CHOH]+, [(CH3)2COH]+, CH3OH and [CH2OCH3]+ and the C–N bond in [CH2NH2]+. Trends in these data are consistent with appreciable π-bonding only in the C–O connections which correspond to the C=O bond in the parent aldehyde or ketone from which the oxonium ion may be considered to be derived by protonation or alkyl cationation.}
}

@article{doi:10.1177/1548512916681672,
author = {Aaron B Frank},
title = {Toward computational net assessment},
journal = {The Journal of Defense Modeling and Simulation},
volume = {14},
number = {1},
pages = {79–94},
year = {2017c},
doi = {10.1177/1548512916681672},
URL = {https://doi-org.crai.referencistas.com/10.1177/1548512916681672},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1548512916681672},
abstract = {a. In 1973, the Department of Defense (DoD) created the Office of Net Assessment (ONA) with a charter and unique approach to strategic analysis. This approach questioned the suitability of systems analysis to assess long-term, dynamic competition between complex military organizations, and turned to more qualitative methods as analytic alternatives. Developments in computing technology and modeling methods over the last two decades, most notably agent-based modeling (ABM), provide new opportunities to address the central analytic questions that motivated the original development of net assessment as a distinctive practice of strategic analysis. By employing ABM to simulate and analyze the behavior of strategic, adaptive, boundedly rational actors, which have previously frustrated mathematical analysis, a new generation of computational models can provide opportunities to add rigor to net assessment.}
}

@article{doi:10.1177/039139880102400306,
author = {L. Goubergrits and K. Affeld and E. Wellnhofer and R. Zurbrügg and T. Holmer},
title = {Estimation of wall shear stress in bypass grafts with computational fluid dynamics method},
journal = {The International Journal of Artificial Organs},
volume = {24},
number = {3},
pages = {145–151},
year = {2001d},
doi = {10.1177/039139880102400306},
note = {PMID:11314808},
URL = {https://doi-org.crai.referencistas.com/10.1177/039139880102400306},
eprint = {https://doi-org.crai.referencistas.com/10.1177/039139880102400306},
abstract = {Coronary artery bypass graft (CABG) operation for coronary artery disease with different types of grafts has a large clinical application world wide. Immediately after this operation patients are usually relieved of their chest pain and have improved cardiac function. However, after a while, these bypass grafts may fail due to for example, neointimal hyperplasia or thrombosis. One of the causes for this bypass graft failure is assumed to be the blood flow with low wall shear stress. The aim of this research is to estimate the wall shear stress in a graft and thus to locate areas were wall shear stress is low. This was done with the help of a blood flow computer model. Postoperative biplane angiograms of the graft were recorded, and from these the three-dimensional geometry of the graft was reconstructed and imported into the computational fluid dynamics (CFD) program FLUENT. The stationary diastolic flow through the grafts was calculated, and the wall shear stress distribution was estimated. This procedure was carried out for one native vessel and two different types of bypass grafts. One bypass graft was a saphenous vein and the other one was a varicose saphenous vein encased in a fine, flexible metal mesh. The mesh was attached to give the graft a defined diameter. The computational results show that each graft has distinct areas of low wall shear stress. The graft with the metal mesh has an area of low wall shear stress (< 1 Pa, stationary flow), which is four times smaller than the respective areas in the other graft and in the native vessel. This is thought to be caused by the smaller and more uniform diameter of the metal mesh-reinforced graft.}
}

@article{doi:10.1177/0092055X211033632,
author = {Amy L. Johnson and Rebecca D. Gleit},
title = {Teaching for a Data-Driven Future: Intentionally Building Foundational Computing Skills},
journal = {Teaching Sociology},
volume = {50},
number = {1},
pages = {49–61},
year = {2022e},
doi = {10.1177/0092055X211033632},
URL = {https://doi-org.crai.referencistas.com/10.1177/0092055X211033632},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0092055X211033632},
abstract = {Despite the centrality of data analysis to the discipline, sociology departments are currently falling short of teaching both undergraduate and graduate students crucial computing and statistical software skills. We argue that sociology instructors must intentionally and explicitly teach computing skills alongside statistical concepts to prepare their students for participation in a data-driven world. We illuminate foundational concepts for computing in the social sciences and provide easy-to-integrate recommendations for building competency with these concepts in the form of a workshop designed to introduce sociology undergraduate and graduate students to the logic of statistical software. We use our workshop to show that students appreciate and gain confidence from being taught how to think about computing.}
}

@article{doi:10.1177/0960327115605440,
author = {RT Naven and S Louise-May},
title = {Computational toxicology: Its essential role in reducing drug attrition},
journal = {Human & Experimental Toxicology},
volume = {34},
number = {12},
pages = {1304–1309},
year = {2015f},
doi = {10.1177/0960327115605440},
note = {PMID:26614820},
URL = {https://doi-org.crai.referencistas.com/10.1177/0960327115605440},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0960327115605440},
abstract = {Predictive toxicology plays a critical role in reducing the failure rate of new drugs in pharmaceutical research and development. Despite recent gains in our understanding of drug-induced toxicity, however, it is urgent that the utility and limitations of our current predictive tools be determined in order to identify gaps in our understanding of mechanistic and chemical toxicology. Using recently published computational regression analyses of in vitro and in vivo toxicology data, it will be demonstrated that significant gaps remain in early safety screening paradigms. More strategic analyses of these data sets will allow for a better understanding of their domain of applicability and help identify those compounds that cause significant in vivo toxicity but which are currently mis-predicted by in silico and in vitro models. These ‘outliers’ and falsely predicted compounds are metaphorical lighthouses that shine light on existing toxicological knowledge gaps, and it is essential that these compounds are investigated if attrition is to be reduced significantly in the future. As such, the modern computational toxicologist is more productively engaged in understanding these gaps and driving investigative toxicology towards addressing them.}
}

@article{doi:10.1080/09506608.2023.2169501,
author = {Shashank Sharma and Sameehan S. Joshi and Mangesh V. Pantawane and Madhavan Radhakrishnan and Sangram Mazumder and Narendra B. Dahotre},
title = {Multiphysics multi-scale computational framework for linking process–structure–property relationships in metal additive manufacturing: a critical review},
journal = {International Materials Reviews},
volume = {68},
number = {7},
pages = {943–1009},
year = {2023g},
doi = {10.1080/09506608.2023.2169501},
URL = {https://doi-org.crai.referencistas.com/10.1080/09506608.2023.2169501},
eprint = {https://doi-org.crai.referencistas.com/10.1080/09506608.2023.2169501},
abstract = {This review article provides a critical assessment of the progress made in computational modelling of metal-based additive manufacturing (AM) with emphasis on its ability to predict physical phenomena, concepts of microstructural evolution, residual stresses, role of multiple thermal cycles, and formation of multi-dimensional defects along with the achieved degree of experimental validation. The uniqueness of this article stems from the inclusion of comprehensive information on computational progress in the field of fusion-based, sintering-based, and mechanical deformation-based AM. A computational model’s role in determining the process framework for the desired outcome of the set properties of the AM components is recognised while presenting the process-microstructure maps, thereby appraising computational ability towards the qualification of products. The inclusion of a detailed discussion on the bi-directional coupling of machine learning and physics-based computational models provides a futuristic roadmap for the digital twin of metal-based AM.}
}

@article{doi:10.1177/1094342016677599,
author = {Miguel A Vega-Rodríguez and Álvaro Rubio-Largo},
title = {Parallelism in computational biology: A view from diverse high-performance computing applications},
journal = {The International Journal of High Performance Computing Applications},
volume = {32},
number = {3},
pages = {317–320},
year = {2018h},
doi = {10.1177/1094342016677599},
URL = {https://doi-org.crai.referencistas.com/10.1177/1094342016677599},
eprint = {https://doi-org.crai.referencistas.com/10.1177/1094342016677599},
abstract = {Computational biology allows and encourages the application of many different parallelism-based technologies. This special issue brings together high-quality state-of-the-art contributions about parallelism-based technologies in computational biology, from different points of view or perspectives, that is, from diverse high-performance computing applications. The special issue collects considerably extended and improved versions of the best papers, accepted and presented in PBio 2015 (the Third International Workshop on Parallelism in Bioinformatics, and part of IEEE ISPA 2015). The domains and topics covered in these seven papers are timely and important, and the authors have done an excellent job of presenting the material.}
}

@article{doi:10.3233/JAD-201011,
author = {Ying Zhang and Yajing Hao and Lang Li and Kai Xia and Guorong Wu},
title = {A Novel Computational Proxy for Characterizing Cognitive Reserve in Alzheimer’s Disease},
journal = {Journal of Alzheimer’s Disease},
volume = {78},
number = {3},
pages = {1217–1228},
year = {2020i},
doi = {10.3233/JAD-201011},
URL = {https://doi-org.crai.referencistas.com/10.3233/JAD-201011},
eprint = {https://doi-org.crai.referencistas.com/10.3233/JAD-201011},
abstract = {Background: Although the abnormal depositions of amyloid plaques and neurofibrillary tangles are the hallmark of Alzheimer’s disease (AD), converging evidence shows that the individual’s neurodegeneration trajectory is regulated by the brain’s capability to maintain normal cognition. Objective: The concept of cognitive reserve has been introduced into the field of neuroscience, acting as a moderating factor for explaining the paradoxical relationship between the burden of AD pathology and the clinical outcome. It is of high demand to quantify the degree of conceptual cognitive reserve on an individual basis. Methods: We propose a novel statistical model to quantify an individual’s cognitive reserve against neuropathological burdens, where the predictors include demographic data (such as age and gender), socioeconomic factors (such as education and occupation), cerebrospinal fluid biomarkers, and AD-related polygenetic risk score. We conceptualize cognitive reserve as a joint product of AD pathology and socioeconomic factors where their interaction manifests a significant role in counteracting the progression of AD in our statistical model. Results: We apply our statistical models to re-investigate the moderated neurodegeneration trajectory by considering cognitive reserve, where we have discovered that 1) high education individuals have significantly higher reserve against the neuropathology than the low education group; however, 2) the cognitive decline in the high education group is significantly faster than low education individuals after the level of pathological burden increases beyond the tipping point. Conclusion: We propose a computational proxy of cognitive reserve that can be used in clinical routine to assess the progression of AD.}
}

@article{doi:10.1177/0963721420915873,
author = {Johannes C. Ziegler and Conrad Perry and Marco Zorzi},
title = {Learning to Read and Dyslexia: From Theory to Intervention Through Personalized Computational Models},
journal = {Current Directions in Psychological Science},
volume = {29},
number = {3},
pages = {293–300},
year = {2020j},
doi = {10.1177/0963721420915873},
note = {PMID:32655213},
URL = {https://doi-org.crai.referencistas.com/10.1177/0963721420915873},
eprint = {https://doi-org.crai.referencistas.com/10.1177/0963721420915873},
abstract = {How do children learn to read? How do deficits in various components of the reading network affect learning outcomes? How does remediating one or several components change reading performance? In this article, we summarize what is known about learning to read and how this can be formalized in a developmentally plausible computational model of reading acquisition. The model is used to understand normal and impaired reading development (dyslexia). In particular, we show that it is possible to simulate individual learning trajectories and intervention outcomes on the basis of three component skills: orthography, phonology, and vocabulary. We therefore advocate a multifactorial computational approach to understanding reading that has practical implications for dyslexia and intervention.}
}

